{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5512c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "\n",
    "# Set the input text\n",
    "text = \"\"\"Several thousand years ago in north-central India, two people sat in a chariot in the midpoint of a great battlefield. \n",
    "        One of them, the yogi Arjuna, knew that it would not be long before the conflict would begin. \n",
    "        So he asked Krishna, the Master of Yoga, what should be his attitude and perspective in this moment. \n",
    "        And above all: What should he do? There was no time to spare in empty words. \n",
    "        In a brief discourse, later turned into seven-hundred Sanskrit verses by the sage Vyasa, Krishna outlined to Arjuna the way to live an entire \n",
    "        life so as to gain perfect self-knowledge and self-mastery: The Bhagavad Gita.  \n",
    "        The Bhagavad Gita tells us that we can attain a Knowing beyond even what it tells us. \n",
    "        And it shows us the way. \n",
    "        In The Bhagavad Gita for Awakening, Abbot George Burke offers a practical commentary for leading a successful spiritual life. \n",
    "        With penetrating insight, he illumines the Bhagavad Gita’s \n",
    "        practical value for spiritual seekers, and the timelessness of India’s most beloved scripture.\"\"\"\n",
    "\n",
    "\n",
    "# Define a function for summarizing the input text\n",
    "def summarizer(rawdocs):\n",
    "    \n",
    "    # Create a list of stop words for English language\n",
    "    stopwords = list(STOP_WORDS)\n",
    "    \n",
    "    # Load the pre-trained English language model from spaCy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # Parse the input text using spaCy\n",
    "    doc = nlp(rawdocs)\n",
    "    \n",
    "    # Create a list of tokens from the parsed input text\n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "    # Create a dictionary of word frequency count for each word in the parsed input text\n",
    "    word_frq = {}\n",
    "    for word in doc:\n",
    "        if word.text.lower() not in stopwords and word.text.lower() not in punctuation:\n",
    "            if word.text not in word_frq.keys():\n",
    "                word_frq[word.text] = 1\n",
    "            else:\n",
    "                word_frq[word.text] += 1\n",
    "\n",
    "    # Normalize the word frequency count dictionary to values between 0 and 1\n",
    "    max_frq = max(word_frq.values())\n",
    "    for word in word_frq.keys():\n",
    "        word_frq[word] =  word_frq[word]/max_frq\n",
    "        \n",
    "    # Create a list of sentence tokens from the parsed input text\n",
    "    sent_tokens = [sent for sent in doc.sents]\n",
    "    \n",
    "    # Create a dictionary of sentence scores by summing the normalized frequency count of words present in each sentence\n",
    "    sent_scores = {}\n",
    "    for sent in sent_tokens:\n",
    "        for word in sent:\n",
    "            if word.text in word_frq.keys():\n",
    "                if sent not in sent_scores.keys():\n",
    "                    sent_scores[sent] = word_frq[word.text]\n",
    "                else:\n",
    "                    sent_scores[sent] += word_frq[word.text]\n",
    "\n",
    "    # Calculate the length of the summary to be generated as 30% of the total number of sentences\n",
    "    select_len = int(len(sent_tokens) * 0.3)\n",
    "    \n",
    "    # Extract the top n sentences with the highest scores from the sentence scores dictionary\n",
    "    summary = nlargest(select_len, sent_scores, key = sent_scores.get)\n",
    "    \n",
    "    # Convert the selected sentences into a list of words\n",
    "    final_summary = [word.text for word in summary]\n",
    "    \n",
    "    # Join the list of words into a single string representing the summary of the input text\n",
    "    summary = \" \".join(final_summary)\n",
    "    \n",
    "    # Return the summary along with the parsed\n",
    "    return summary, doc, len(rawdocs.split(' ')), len(summary.split(' '))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
